{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# RAG Generator (Transformers + BitsAndBytes)\n",
				"## –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è 2x RTX 5090\n",
				"–ú–æ–¥–µ–ª—å: Qwen/Qwen2.5-32B-Instruct (4-bit NF4)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–æ–≤\n",
				"!pip install -q transformers accelerate bitsandbytes qdrant-client pandas sentencepiece protobuf tqdm"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =====\n",
				"MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
				"\n",
				"# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
				"BATCH_SIZE = 16          # –ú–æ–∂–Ω–æ –±–æ–ª—å—à–µ –¥–ª—è 7B –º–æ–¥–µ–ª–∏\n",
				"MAX_NEW_TOKENS = 200\n",
				"TEMPERATURE = 0.1\n",
				"\n",
				"# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
				"DATA_DIR = \"data\"\n",
				"QUESTIONS_CSV = os.path.join(DATA_DIR, \"questions_clean.csv\")\n",
				"IDS_CSV = \"final/submission_ids.csv\"\n",
				"OUTPUT_CSV = \"final/final_su.csv\"\n",
				"\n",
				"# Qdrant\n",
				"COLLECTION_NAME = \"documents1\"\n",
				"QDRANT_URL = \"http://158.160.208.30:6333\"\n",
				"\n",
				"# –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (True = —Ç–æ–ª—å–∫–æ 20 –≤–æ–ø—Ä–æ—Å–æ–≤)\n",
				"TEST_MODE = False\n",
				"TEST_LIMIT = 20\n",
				"\n",
				"SYSTEM_PROMPT = \"\"\"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫–∞.\n",
				"–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n",
				"–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –Ω–∞–ø–∏—à–∏: \"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\".\n",
				"\n",
				"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n",
				"1. –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n",
				"2. –û—Ç–≤–µ—Ç: 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫—Ä–∞—Ç–∫–æ.\n",
				"3. –Ø–∑—ã–∫: –†—É—Å—Å–∫–∏–π.\n",
				"\"\"\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –§–£–ù–ö–¶–ò–ò –†–ê–ë–û–¢–´ –° QDRANT =====\n",
				"\n",
				"async def fetch_single_doc(client, doc_id: str):\n",
				"    try:\n",
				"        points, _ = await client.scroll(\n",
				"            collection_name=COLLECTION_NAME,\n",
				"            scroll_filter=models.Filter(\n",
				"                must=[models.FieldCondition(key=\"doc_id\", match=models.MatchValue(value=str(doc_id)))]\n",
				"            ),\n",
				"            limit=20,\n",
				"            with_payload=True,\n",
				"            with_vectors=False\n",
				"        )\n",
				"        if points:\n",
				"            return str(doc_id), \"\\n\".join([p.payload.get('text', '') for p in points])\n",
				"    except:\n",
				"        pass\n",
				"    return str(doc_id), \"\"\n",
				"\n",
				"async def prefetch_documents(all_doc_ids):\n",
				"    print(f\"üì• –ó–∞–≥—Ä—É–∑–∫–∞ {len(all_doc_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Qdrant...\")\n",
				"    client = AsyncQdrantClient(url=QDRANT_URL)\n",
				"    doc_cache = {}\n",
				"    batch_size = 200\n",
				"    ids_list = list(all_doc_ids)\n",
				"    \n",
				"    for i in tqdm(range(0, len(ids_list), batch_size), desc=\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\"):\n",
				"        batch = ids_list[i:i+batch_size]\n",
				"        results = await asyncio.gather(*[fetch_single_doc(client, d_id) for d_id in batch])\n",
				"        for d_id, text in results:\n",
				"            doc_cache[d_id] = text\n",
				"        \n",
				"    await client.close()\n",
				"    print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(doc_cache)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
				"    return doc_cache"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• =====\n",
				"\n",
				"def prepare_data(df, doc_cache):\n",
				"    prompts = []\n",
				"    q_ids = []\n",
				"    \n",
				"    for _, row in df.iterrows():\n",
				"        q_id = row['q_id']\n",
				"        query = row['query']\n",
				"        \n",
				"        try:\n",
				"            doc_ids = ast.literal_eval(str(row.get('retrieved_ids', '[]')))\n",
				"            if not isinstance(doc_ids, list): doc_ids = []\n",
				"        except:\n",
				"            doc_ids = []\n",
				"            \n",
				"        context_texts = [doc_cache.get(str(d_id), \"\") for d_id in doc_ids if doc_cache.get(str(d_id))]\n",
				"        full_context = \"\\n\\n\".join(context_texts)[:10000]\n",
				"        \n",
				"        if not full_context.strip():\n",
				"            full_context = \"–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\"\n",
				"        \n",
				"        messages = [\n",
				"            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
				"            {\"role\": \"user\", \"content\": f\"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\\n{full_context}\\n\\n–í–æ–ø—Ä–æ—Å: {query}\"}\n",
				"        ]\n",
				"        prompts.append(messages)\n",
				"        q_ids.append(q_id)\n",
				"        \n",
				"    return prompts, q_ids"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò =====\n",
				"\n",
				"print(f\"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {MODEL_NAME}...\")\n",
				"\n",
				"bnb_config = BitsAndBytesConfig(\n",
				"    load_in_4bit=True,\n",
				"    bnb_4bit_quant_type=\"nf4\",\n",
				"    bnb_4bit_compute_dtype=torch.float16,\n",
				"    bnb_4bit_use_double_quant=True,\n",
				")\n",
				"\n",
				"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
				"tokenizer.padding_side = \"left\"\n",
				"if tokenizer.pad_token is None:\n",
				"    tokenizer.pad_token = tokenizer.eos_token\n",
				"\n",
				"model = AutoModelForCausalLM.from_pretrained(\n",
				"    MODEL_NAME,\n",
				"    quantization_config=bnb_config,\n",
				"    device_map=\"auto\",\n",
				"    trust_remote_code=True,\n",
				"    torch_dtype=torch.float16,\n",
				")\n",
				"model.eval()\n",
				"\n",
				"print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• =====\n",
				"\n",
				"print(\"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
				"\n",
				"q_df = pd.read_csv(QUESTIONS_CSV)\n",
				"ids_df = pd.read_csv(IDS_CSV)\n",
				"df = pd.merge(q_df, ids_df, on='q_id', how='left')\n",
				"\n",
				"if TEST_MODE:\n",
				"    print(f\"‚ö†Ô∏è –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: {TEST_LIMIT} –≤–æ–ø—Ä–æ—Å–æ–≤\")\n",
				"    df = df.head(TEST_LIMIT)\n",
				"\n",
				"print(f\"–í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(df)}\")\n",
				"\n",
				"# –°–±–æ—Ä ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
				"all_doc_ids = set()\n",
				"for _, row in df.iterrows():\n",
				"    try:\n",
				"        d_ids = ast.literal_eval(str(row.get('retrieved_ids', '[]')))\n",
				"        if isinstance(d_ids, list):\n",
				"            all_doc_ids.update(str(x) for x in d_ids)\n",
				"    except: pass\n",
				"\n",
				"print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_doc_ids)}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í –ò–ó QDRANT =====\n",
				"\n",
				"doc_cache = await prefetch_documents(all_doc_ids)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ü–û–î–ì–û–¢–û–í–ö–ê –ü–†–û–ú–ü–¢–û–í =====\n",
				"\n",
				"prompts, q_ids = prepare_data(df, doc_cache)\n",
				"print(f\"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤\")\n",
				"\n",
				"# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
				"text_prompts = [\n",
				"    tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)\n",
				"    for p in prompts\n",
				"]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–û–í =====\n",
				"\n",
				"all_answers = []\n",
				"total = len(text_prompts)\n",
				"\n",
				"print(f\"üî• –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ({total} –∑–∞–ø—Ä–æ—Å–æ–≤, batch={BATCH_SIZE})...\")\n",
				"\n",
				"for i in tqdm(range(0, total, BATCH_SIZE), desc=\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è\"):\n",
				"    batch_prompts = text_prompts[i : i + BATCH_SIZE]\n",
				"    \n",
				"    inputs = tokenizer(\n",
				"        batch_prompts, \n",
				"        return_tensors=\"pt\", \n",
				"        padding=True, \n",
				"        truncation=True, \n",
				"        max_length=8192\n",
				"    ).to(\"cuda\")\n",
				"    \n",
				"    with torch.no_grad():\n",
				"        outputs = model.generate(\n",
				"            **inputs,\n",
				"            max_new_tokens=MAX_NEW_TOKENS,\n",
				"            temperature=TEMPERATURE,\n",
				"            do_sample=False,\n",
				"            pad_token_id=tokenizer.pad_token_id,\n",
				"            eos_token_id=tokenizer.eos_token_id,\n",
				"        )\n",
				"    \n",
				"    input_len = inputs.input_ids.shape[1]\n",
				"    generated_tokens = outputs[:, input_len:]\n",
				"    batch_answers = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
				"    all_answers.extend(batch_answers)\n",
				"\n",
				"print(f\"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(all_answers)} –æ—Ç–≤–µ—Ç–æ–≤\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í =====\n",
				"\n",
				"results = []\n",
				"for q_id, ans in zip(q_ids, all_answers):\n",
				"    ans_clean = ans.strip()\n",
				"    # –§–∏–ª—å—Ç—Ä –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤\n",
				"    if \"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\" in ans_clean or any(\"\\u4e00\" <= c <= \"\\u9fff\" for c in ans_clean):\n",
				"        ans_clean = \"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\"\n",
				"    results.append({\"q_id\": q_id, \"answer\": ans_clean})\n",
				"\n",
				"final_df = pd.DataFrame(results)\n",
				"final_df.to_csv(OUTPUT_CSV, index=False)\n",
				"\n",
				"print(f\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_CSV}\")\n",
				"print(f\"üìä –í—Å–µ–≥–æ: {len(final_df)}\")\n",
				"print(f\"üìä '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ': {len(final_df[final_df['answer'].str.contains('–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ', na=False)])}\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ===== –ü–†–û–°–ú–û–¢–† –†–ï–ó–£–õ–¨–¢–ê–¢–û–í =====\n",
				"\n",
				"final_df.head(10)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.0"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
