"""
Direct High-Performance Generator for 2x RTX 5090
Uses vLLM offline inference engine directly (no server required).
Model: Qwen2.5-72B-Instruct-AWQ (Max quality/speed balance for 2x32GB)
"""

import os
import ast
import asyncio
import pandas as pd
from qdrant_client import AsyncQdrantClient, models
from vllm import LLM, SamplingParams

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
# –ò—Å–ø–æ–ª—å–∑—É–µ–º 72B –º–æ–¥–µ–ª—å, —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å 64GB VRAM. –û–Ω–∞ –≤–ª–µ–∑–∞–µ—Ç –≤ Int4 (AWQ).
MODEL_NAME = "Qwen/Qwen2.5-72B-Instruct-AWQ"
TENSOR_PARALLEL_SIZE = 2  # –†–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏–µ –Ω–∞ 2 –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã
MAX_MODEL_LEN = 16384     # –£ Qwen –æ–≥—Ä–æ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –º–æ–∂–µ–º –ø–æ–∑–≤–æ–ª–∏—Ç—å –±–æ–ª—å—à–µ
GPU_MEM_UTIL = 0.95       # –ó–∞–±–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ –º–∞–∫—Å–∏–º—É–º—É

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
SAMPLING_PARAMS = SamplingParams(
    temperature=0.1,
    max_tokens=300,        # –û—Ç–≤–µ—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ
    min_p=0.05,
    repetition_penalty=1.1,
    stop=["<|endoftext|>", "<|im_end|>"]
)

# –ü—É—Ç–∏ (–ø–æ–ø—Ä–∞–≤—å—Ç–µ –ø–æ–¥ —Å–≤–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
DATA_DIR = os.getenv("DATA_DIR", "data")
QUESTIONS_CSV = os.path.join(DATA_DIR, "questions_clean.csv")
IDS_CSV = "final/submission_ids.csv"
OUTPUT_CSV = "final/final_su.csv"
COLLECTION_NAME = "documents1"
QDRANT_URL = "http://localhost:6333" # Qdrant –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–ø—É—â–µ–Ω (–º–æ–∂–Ω–æ –≤ –¥–æ–∫–µ—Ä–µ –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ)

SYSTEM_PROMPT = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫–∞.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –Ω–∞–ø–∏—à–∏ —Å—Ç—Ä–æ–≥–æ: "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ".

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –ò—Å–ø–æ–ª—å–∑—É–π —Ü–∏—Ç–∞—Ç—ã –∏ —Ñ–∞–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
2. –°—Ç–∏–ª—å: –≤–µ–∂–ª–∏–≤—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ.
4. –Ø–∑—ã–∫: –†—É—Å—Å–∫–∏–π.
"""

# --- –ß–ê–°–¢–¨ 1: –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ Qdrant (Async) ---

async def fetch_single_doc(client, doc_id: str):
    try:
        points, _ = await client.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter=models.Filter(
                must=[models.FieldCondition(key="doc_id", match=models.MatchValue(value=str(doc_id)))]
            ),
            limit=20,
            with_payload=True,
            with_vectors=False
        )
        if points:
            return str(doc_id), "\n".join([p.payload.get('text', '') for p in points])
    except:
        pass
    return str(doc_id), ""

async def prefetch_documents(all_doc_ids):
    print(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {len(all_doc_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Qdrant...")
    client = AsyncQdrantClient(url=QDRANT_URL)
    
    tasks = []
    doc_cache = {}
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏, —á—Ç–æ–±—ã –Ω–µ —É–±–∏—Ç—å —Å–µ—Ç—å/Qdrant
    batch_size = 200
    ids_list = list(all_doc_ids)
    
    for i in range(0, len(ids_list), batch_size):
        batch = ids_list[i:i+batch_size]
        batch_tasks = [fetch_single_doc(client, d_id) for d_id in batch]
        results = await asyncio.gather(*batch_tasks)
        for d_id, text in results:
            doc_cache[d_id] = text
        print(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ {min(i+batch_size, len(ids_list))}/{len(ids_list)}")
        
    await client.close()
    print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –í—Å–µ–≥–æ: {len(doc_cache)}")
    return doc_cache

def prepare_prompts(df, doc_cache):
    print("üìù –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤...")
    prompts = []
    indices = []
    
    for idx, row in df.iterrows():
        q_id = row['q_id']
        query = row['query']
        
        # –ü–æ–ª—É—á–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        try:
            doc_ids = ast.literal_eval(str(row.get('retrieved_ids', '[]')))
            if not isinstance(doc_ids, list): doc_ids = []
        except:
            doc_ids = []
            
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_texts = [doc_cache.get(str(d_id), "") for d_id in doc_ids if doc_cache.get(str(d_id))]
        full_context = "\n\n".join(context_texts)[:20000] # –ë–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è 72B –º–æ–¥–µ–ª–∏
        
        if not full_context.strip():
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–æ–º–µ—á–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥–ª—É—à–∫–∏ (–Ω–æ vllm –ø—Ä–æ–≥–æ–Ω–∏–º –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏–º)
            full_context = "–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        
        # –§–æ—Ä–º–∞—Ç ChatML (Qwen –µ–≥–æ –ª—é–±–∏—Ç)
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{full_context}\n\n–í–æ–ø—Ä–æ—Å: {query}"}
        ]
        
        # –í vLLM –º–æ–∂–Ω–æ –ø–æ–¥–∞–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π, –Ω–æ –ª—É—á—à–µ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        # Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ChatML. vLLM —Å–∞–º –ø—Ä–∏–º–µ–Ω–∏—Ç chat_template –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞—Ç—å tokenizer
        # –ù–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø–µ—Ä–µ–¥–∞–¥–∏–º messages –≤ generate (vLLM >= 0.6.0 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç entry —Å messages)
        # –ï—Å–ª–∏ —Å—Ç–∞—Ä—ã–π vLLM, –Ω—É–∂–Ω–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä—É—á–Ω—É—é. –ë—É–¥–µ–º –Ω–∞–¥–µ—è—Ç—å—Å—è –Ω–∞ —Å–≤–µ–∂–∏–π vLLM.
        # –î–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º apply_chat_template —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ü–û–ó–ñ–ï, 
        # –Ω–æ –ø–æ–∫–∞ —Å–æ–±–µ—Ä–µ–º –ø—Ä–æ—Å—Ç–æ –¥–∞–Ω–Ω—ã–µ.
        
        prompts.append(messages)
        indices.append(idx)
        
    return prompts, indices

# --- –ß–ê–°–¢–¨ 2: –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (Sync) ---

def run_inference(prompts):
    print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {MODEL_NAME} –Ω–∞ {TENSOR_PARALLEL_SIZE} GPU...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞
    llm = LLM(
        model=MODEL_NAME,
        quantization="awq",
        dtype="float16",
        tensor_parallel_size=TENSOR_PARALLEL_SIZE,
        gpu_memory_utilization=GPU_MEM_UTIL,
        max_model_len=MAX_MODEL_LEN,
        enforce_eager=False, # True –¥–ª—è torch graph (–±—ã—Å—Ç—Ä–µ–µ), False –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã
        trust_remote_code=True
    )
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    # vLLM –ø—Ä–∏–Ω–∏–º–∞–µ—Ç prompt_token_ids –∏–ª–∏ prompts. 
    # –ù–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Å tokenizer.apply_chat_template.
    # –°–¥–µ–ª–∞–µ–º —ç—Ç–æ —á–µ—Ä–µ–∑ —Å–∞–º LLM.
    
    tokenizer = llm.get_tokenizer()
    text_prompts = [
        tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True) 
        for p in prompts
    ]
    
    print(f"üî• –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è {len(text_prompts)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
    outputs = llm.generate(text_prompts, SAMPLING_PARAMS)
    
    results = []
    for output in outputs:
        generated_text = output.outputs[0].text.strip()
        results.append(generated_text)
        
    return results

# --- MAIN ---

def main():
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(QUESTIONS_CSV) or not os.path.exists(IDS_CSV):
        print("‚ùå –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return

    q_df = pd.read_csv(QUESTIONS_CSV)
    ids_df = pd.read_csv(IDS_CSV)
    df = pd.merge(q_df, ids_df, on='q_id', how='left')
    
    # –¢–ï–°–¢ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è —Ç–µ—Å—Ç–∞)
    # df = df.head(50) 
    
    # 2. –°–±–æ—Ä –≤—Å–µ—Ö ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    all_doc_ids = set()
    for _, row in df.iterrows():
        try:
            d_ids = ast.literal_eval(str(row.get('retrieved_ids', '[]')))
            if isinstance(d_ids, list):
                all_doc_ids.update(str(x) for x in d_ids)
        except: pass
        
    # 3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω—É–∂–µ–Ω –∑–∞–ø—É—â–µ–Ω–Ω—ã–π Qdrant!)
    # Qdrant –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –≤ –¥–æ–∫–µ—Ä–µ: docker compose up -d qdrant
    doc_cache = asyncio.run(prefetch_documents(all_doc_ids))
    
    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
    prompts, indices = prepare_prompts(df, doc_cache)
    
    # 5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
    answers = run_inference(prompts)
    
    # 6. –°–±–æ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_data = []
    for idx, ans in zip(indices, answers):
        q_id = df.loc[idx, 'q_id']
        
        # –§–∏–ª—å—Ç—Ä
        if "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ" in ans or any("\u4e00" <= c <= "\u9fff" for c in ans):
            final_ans = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ"
        else:
            final_ans = ans
            
        results_data.append({"q_id": q_id, "answer": final_ans})
        
    final_df = pd.DataFrame(results_data)
    final_df.to_csv(OUTPUT_CSV, index=False)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
