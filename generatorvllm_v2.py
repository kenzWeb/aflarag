"""
Optimized RAG Generator v3
Target: 200+ tokens/sec throughput
Key optimizations:
1. Batch prefetch all documents BEFORE generation
2. Few-shot prompt for better style matching
3. Parallel document fetching with asyncio.gather
4. Retry decorator for LLM errors
5. Model: Mistral 7B v0.3 AWQ
6. repetition_penalty: 1.1 for quality
"""

import os
import argparse
import asyncio
import re
import pandas as pd
import ast
from functools import wraps
from openai import AsyncOpenAI
from qdrant_client import AsyncQdrantClient, models
from tqdm.asyncio import tqdm

# --- –ö–û–ù–§–ò–ì ---
API_URL = "http://localhost:8000/v1"
API_KEY = "EMPTY"
MODEL_NAME = "microsoft/Phi-3.5-mini-instruct"

# Retry config
MAX_RETRIES = 3
RETRY_DELAY = 0.5

DATA_DIR = os.getenv("DATA_DIR", "data")
QUESTIONS_CSV = os.path.join(DATA_DIR, "questions_clean.csv")
IDS_CSV = "final/submission_ids.csv"
OUTPUT_CSV = "final/final_su.csv"

COLLECTION_NAME = "documents1"

# –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º concurrency, —Ç.–∫. prefetch —É–±–∏—Ä–∞–µ—Ç bottleneck
CONCURRENT_REQUESTS = 256
QDRANT_BATCH_SIZE = 100  # Batch fetching from Qdrant

# –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
TEST_MODE = False
TEST_LIMIT = 50

# –ö–ª–∏–µ–Ω—Ç—ã
client_qdrant = AsyncQdrantClient(url="http://localhost:6333")
aclient = AsyncOpenAI(base_url=API_URL, api_key=API_KEY)

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ç–∏–ª—è
SYSTEM_PROMPT = """–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫–∞. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞ —Å—Ç—Ä–æ–≥–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

–ü—Ä–∞–≤–∏–ª–∞:
- –û—Ç–≤–µ—Ç: 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
- –ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Üí –æ—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ: "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ"
- –Ø–∑—ã–∫: —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π

–ü—Ä–∏–º–µ—Ä—ã:
–í–æ–ø—Ä–æ—Å: –ö–∞–∫ —É–∑–Ω–∞—Ç—å –Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞?
–û—Ç–≤–µ—Ç: –ù–æ–º–µ—Ä —Å—á–µ—Ç–∞ —É–∫–∞–∑–∞–Ω –≤ —Ä–∞–∑–¥–µ–ª–µ –¥–æ–≥–æ–≤–æ—Ä–∞ —Å —Ä–µ–∫–≤–∏–∑–∏—Ç–∞–º–∏. –¢–∞–∫–∂–µ –µ–≥–æ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ –º–æ–±–∏–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ "–°—á–µ—Ç–∞".

–í–æ–ø—Ä–æ—Å: –ü–æ—á–µ–º—É –Ω–µ –ø—Ä–∏—Ö–æ–¥—è—Ç –°–ú–°?
–û—Ç–≤–µ—Ç: –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –≤–∫–ª—é—á–∏–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–æ–¥ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –≤ –ø—Ä–æ—Ñ–∏–ª–µ."""


def async_retry(max_retries=MAX_RETRIES, delay=RETRY_DELAY):
    """Decorator for async retry on failure"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_error = None
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_error = e
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay * (attempt + 1))
            raise last_error
        return wrapper
    return decorator


async def fetch_single_doc(doc_id: str) -> tuple[str, str]:
    """Fetch single document, return (doc_id, text)"""
    try:
        points, _ = await client_qdrant.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="doc_id",
                        match=models.MatchValue(value=doc_id)
                    )
                ]
            ),
            limit=20,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            with_payload=True,
            with_vectors=False
        )
        if points:
            texts = [p.payload.get('text', '') for p in points]
            return (doc_id, "\n".join(texts))
        return (doc_id, "")
    except Exception:
        return (doc_id, "")


async def prefetch_all_documents(all_doc_ids: set[str]) -> dict[str, str]:
    """
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 2: Prefetch ALL documents in parallel BEFORE generation.
    This eliminates per-request Qdrant latency.
    """
    print(f"üì• Prefetching {len(all_doc_ids)} unique documents...")
    
    # Batch into chunks to avoid overwhelming Qdrant
    doc_list = list(all_doc_ids)
    cache = {}
    
    for i in range(0, len(doc_list), QDRANT_BATCH_SIZE):
        batch = doc_list[i:i + QDRANT_BATCH_SIZE]
        tasks = [fetch_single_doc(doc_id) for doc_id in batch]
        results = await asyncio.gather(*tasks)
        for doc_id, text in results:
            cache[doc_id] = text
        
        # Progress indicator
        if (i + QDRANT_BATCH_SIZE) % 500 == 0:
            print(f"  Fetched {min(i + QDRANT_BATCH_SIZE, len(doc_list))}/{len(doc_list)}")
    
    print(f"‚úÖ Prefetch complete. Cache size: {len(cache)}")
    return cache


async def process_row(row, doc_cache: dict, semaphore: asyncio.Semaphore):
    """Process single question - now cache is pre-populated"""
    async with semaphore:
        q_id = row['q_id']
        query = row['query']
        ids_str = str(row.get('retrieved_ids', '[]'))
        
        try:
            doc_ids = ast.literal_eval(ids_str)
            if not isinstance(doc_ids, list):
                doc_ids = []
        except:
            doc_ids = []
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 3: –ü—Ä–æ—Å—Ç–æ —á–∏—Ç–∞–µ–º –∏–∑ –∫—ç—à–∞, –Ω–∏–∫–∞–∫–∏—Ö await
        context_parts = []
        for d_id in doc_ids:
            cache_key = str(d_id)
            text = doc_cache.get(cache_key, "")
            if text:
                context_parts.append(text)
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 4: –ú–µ–Ω—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ = –±—ã—Å—Ç—Ä–µ–µ prefill
        # 4000 —Å–∏–º–≤–æ–ª–æ–≤ ~ 1200-1500 —Ç–æ–∫–µ–Ω–æ–≤ ‚Äî –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        full_context = "\n\n".join(context_parts)[:5000]
        
        if not full_context.strip():
            return {"q_id": q_id, "answer": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ"}
        
        try:
            ans = await generate_answer(query, full_context)
            return {"q_id": q_id, "answer": ans}
        except Exception as e:
            print(f"LLM Error q_id={q_id}: {e}")
            return {"q_id": q_id, "answer": "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"}


@async_retry(max_retries=MAX_RETRIES)
async def generate_answer(query: str, context: str) -> str:
    """Generate answer with retry logic"""
    response = await aclient.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {query}"}
        ],
        temperature=0.1,
        max_tokens=120,
        extra_body={"min_p": 0.05, "repetition_penalty": 1.1}
    )
    ans = response.choices[0].message.content.strip()
    
    # –§–∏–ª—å—Ç—Ä –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ
    if any("\u4e00" <= char <= "\u9fff" for char in ans):
        return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ"
    
    return ans


async def main(test_mode=False):
    print("=== OPTIMIZED RAG GENERATOR v3 ===")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(QUESTIONS_CSV) or not os.path.exists(IDS_CSV):
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã")
        return

    q_df = pd.read_csv(QUESTIONS_CSV)
    ids_df = pd.read_csv(IDS_CSV)
    
    df = pd.merge(q_df, ids_df, on='q_id', how='left')
    if 'answer' in df.columns and 'retrieved_ids' not in df.columns:
        df.rename(columns={'answer': 'retrieved_ids'}, inplace=True)
    
    if test_mode:
        print(f"‚ö†Ô∏è –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: {TEST_LIMIT} –≤–æ–ø—Ä–æ—Å–æ–≤")
        df = df.head(TEST_LIMIT)
    
    print(f"–í–æ–ø—Ä–æ—Å–æ–≤ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(df)}")
    
    # 2. –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ doc_ids –∏ prefetch'–∏–º –∏—Ö
    all_doc_ids = set()
    for _, row in df.iterrows():
        ids_str = str(row.get('retrieved_ids', '[]'))
        try:
            doc_ids = ast.literal_eval(ids_str)
            if isinstance(doc_ids, list):
                all_doc_ids.update(str(d) for d in doc_ids)
        except:
            pass
    
    # Prefetch all documents
    doc_cache = await prefetch_all_documents(all_doc_ids)
    
    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (—Ç–µ–ø–µ—Ä—å –±–µ–∑ Qdrant latency)
    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)
    tasks = [process_row(row, doc_cache, semaphore) for _, row in df.iterrows()]
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å concurrency={CONCURRENT_REQUESTS}")
    results = await tqdm.gather(*tasks, desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è")
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_df = pd.DataFrame(results).sort_values(by='q_id')
    empty_count = len(final_df[final_df['answer'].str.contains("–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ", na=False)])
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –í—Å–µ–≥–æ {len(final_df)}, '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ': {empty_count}")
    
    final_df.to_csv(OUTPUT_CSV, index=False)
    print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_CSV}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--test", "--—Ç–µ—Å—Ç", action="store_true")
    args = parser.parse_args()
    
    try:
        asyncio.run(main(test_mode=args.test))
    except KeyboardInterrupt:
        print("–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
